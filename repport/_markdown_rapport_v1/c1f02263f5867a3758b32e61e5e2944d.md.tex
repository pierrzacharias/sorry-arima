--- title: "Sorry ARIMA, but I'm Going Bayesian" output: pdf\markdownRendererEmphasis{document editor}options: \markdownRendererInterblockSeparator
{}\markdownRendererHeadingTwo{ chunk\markdownRendererEmphasis{output}type: console}\markdownRendererInterblockSeparator
{}```\markdownRendererLeftBrace{}r setup, include=FALSE\markdownRendererRightBrace{} Sys.setenv(RSTUDIO\markdownRendererEmphasis{PANDOC="/usr/lib/rstudio/bin/pandoc") knitr::opts}chunk\markdownRendererDollarSign{}set(echo = TRUE)\markdownRendererInterblockSeparator
{}```\markdownRendererInterblockSeparator
{}When people think of "data science" they probably think of algorithms that scan large datasets to predict a customer's next move or interpret unstructured text. But what about models that utilize small, time-stamped datasets to forecast dry metrics such as demand and sales? Yes, I'm talking about good old time series analysis, an ancient discipline that hasn't received the cool "data science" re-branding enjoyed by many other areas of analytics.\markdownRendererInterblockSeparator
{}Yet, analysis of time series data presents some of the most difficult analytical challenges: you typically have the least amount of data to work with, while needing to inform some of the most important decisions. For example, time series analysis is frequently used to do demand forecasting for corporate planning, which requires an understanding of seasonality and trend, as well as quantifying the impact of known business drivers. But herein lies the problem: you rarely have sufficient historical data to estimate these components with great precision. And, to make matters worse, validation is more difficult for time series models than it is for classifiers and your audience may not be comfortable with the imbedded uncertainty.\markdownRendererInterblockSeparator
{}So, how does one navigate such treacherous waters? You need business acumen, luck, and \markdownRendererEmphasis{Bayesian structural time series models}. In my opinion, these models are more transparent than ARIMA â€“ which still tends to be the go-to method. They also facilitate better handling of uncertainty, a key feature when planning for the future. In this post I will provide a gentle intro the \markdownRendererCodeSpan{bsts} R package written by Steven L. Scott at Google. Note that the \markdownRendererCodeSpan{bsts} package is also being used by the \markdownRendererCodeSpan{CausalImpact} package written by Kay Brodersen, which I discussed in this \markdownRendererLink{post from January}{http://multithreaded.stitchfix.com/blog/2016/01/13/market-watch/}{http://multithreaded.stitchfix.com/blog/2016/01/13/market-watch/}{}.\markdownRendererInterblockSeparator
{}\markdownRendererHeadingTwo{Airline Passenger Data}\markdownRendererInterblockSeparator
{}\markdownRendererHeadingThree{An ARIMA Model}\markdownRendererInterblockSeparator
{}First, let's start by fitting a classical ARIMA model to the famous airline passenger dataset. The ARIMA model has the following characteristics:\markdownRendererInterblockSeparator
{}\markdownRendererUlBeginTight
\markdownRendererUlItem First order differencing (\markdownRendererDollarSign{}I=1\markdownRendererDollarSign{}) and a moving average term (\markdownRendererDollarSign{}q=1\markdownRendererDollarSign{})\markdownRendererUlItemEnd 
\markdownRendererUlItem Seasonal differencing and a seasonal MA term.\markdownRendererUlItemEnd 
\markdownRendererUlItem The year of 1960 was used as the holdout period for validation.\markdownRendererUlItemEnd 
\markdownRendererUlItem Using a log transformation to model the growth rate.\markdownRendererUlItemEnd 
\markdownRendererUlEndTight \markdownRendererInterblockSeparator
{}```\markdownRendererLeftBrace{}r, echo = TRUE, message=FALSE, eval=TRUE, fig.width=7, fig.height=5\markdownRendererRightBrace{} library(lubridate) library(bsts) library(dplyr) library(ggplot2) library(forecast) library(Boom)\markdownRendererInterblockSeparator
{}data("AirPassengers") Y <- window(AirPassengers, start=c(1949, 1), end=c(1959,12)) arima <- arima(log10(Y), order=c(0, 1, 1), seasonal=list(order=c(0,1,1), period=12))\markdownRendererInterblockSeparator
{}d1 <- data.frame(c(10\markdownRendererCircumflex{}as.numeric(fitted(arima)), \markdownRendererHash{} fitted and predicted 10\markdownRendererCircumflex{}as.numeric(predict(arima, n.ahead = 12)\markdownRendererDollarSign{}pred)), as.numeric(AirPassengers), \markdownRendererHash{}actual values as.Date(time(AirPassengers))) names(d1) <- c("Fitted", "Actual", "Date")\markdownRendererInterblockSeparator
{}MAPE <- filter(d1, year(Date)>1959) \markdownRendererPercentSign{}>\markdownRendererPercentSign{} summarise(MAPE=mean(abs(Actual-Fitted)/Actual))\markdownRendererInterblockSeparator
{}ggplot(data=d1, aes(x=Date)) + geom\markdownRendererEmphasis{line(aes(y=Actual, colour = "Actual"), size=1.2) + geom}line(aes(y=Fitted, colour = "Fitted"), size=1.2, linetype=2) + theme\markdownRendererEmphasis{bw() + theme(legend.title = element}blank()) + ylab("") + xlab("") + geom\markdownRendererEmphasis{vline(xintercept=as.numeric(as.Date("1959-12-01")), linetype=2) + ggtitle(paste0("ARIMA -- Holdout MAPE = ", round(100*MAPE,2), "\markdownRendererPercentSign{}")) + theme(axis.text.x=element}text(angle = -90, hjust = 0)) ```\markdownRendererInterblockSeparator
{}This model predicts the holdout period quite well as measured by the MAPE (mean absolute percentage error). However, the model does not tell us much about the time series itself. In other words, we cannot visualize the "story" of the model. All we know is that we can fit the data well using a combination of moving averages and lagged terms.\markdownRendererInterblockSeparator
{}\markdownRendererHeadingThree{A Bayesian Structural Time Series Model}\markdownRendererInterblockSeparator
{}A different approach would be to use Bayesian structural time series model with unobserved components. This technique is more transparent than ARIMA models and deals with uncertainty in a more elegant manner. It is more transparent because it does not rely on differencing, lags and moving averages to fit the data. You can visually inspect the underlying components of the model. It deals with uncertainty in a better way because you can quantify the posterior uncertainty of the individual components, control the variance of the components, and impose prior beliefs on the model. Last, but not least, any ARIMA model can be recast as a structural model.\markdownRendererInterblockSeparator
{}Generally, we can write a Bayesian structural model like this:\markdownRendererInterblockSeparator
{}\markdownRendererDollarSign{}\markdownRendererDollarSign{} Y\markdownRendererEmphasis{t = \markdownRendererBackslash{}mu}t + x\markdownRendererEmphasis{t \markdownRendererBackslash{}beta + S}t + e\markdownRendererEmphasis{t, e}t \markdownRendererBackslash{}sim N(0, \markdownRendererBackslash{}sigma\markdownRendererCircumflex{}2\markdownRendererEmphasis{e) \markdownRendererDollarSign{}\markdownRendererDollarSign{} \markdownRendererDollarSign{}\markdownRendererDollarSign{} \markdownRendererBackslash{}mu}\markdownRendererLeftBrace{}t+1\markdownRendererRightBrace{} = \markdownRendererBackslash{}mu\markdownRendererEmphasis{t + \markdownRendererBackslash{}nu}t, \markdownRendererBackslash{}nu\markdownRendererEmphasis{t \markdownRendererBackslash{}sim N(0, \markdownRendererBackslash{}sigma\markdownRendererCircumflex{}2}\markdownRendererLeftBrace{}\markdownRendererBackslash{}nu\markdownRendererRightBrace{}). \markdownRendererDollarSign{}\markdownRendererDollarSign{}\markdownRendererInterblockSeparator
{}Here \markdownRendererDollarSign{}x\markdownRendererEmphasis{t\markdownRendererDollarSign{} denotes a set of regressors, \markdownRendererDollarSign{}S}t\markdownRendererDollarSign{} represents seasonality, and \markdownRendererDollarSign{}\markdownRendererBackslash{}mu\markdownRendererUnderscore{}t\markdownRendererDollarSign{} is the \markdownRendererEmphasis{local level} term. The local level term defines how the latent state evolves over time and is often referred to as the \markdownRendererEmphasis{unobserved trend}. Note that the regressor coefficients, seasonality and trend are estimated \markdownRendererEmphasis{simultaneously}, which helps avoid strange coefficient estimates due to spurious relationships (similar in spirit to Granger causality). In addition, this approach facilitates model averaging across many smaller regressions, as well as coefficient shrinkage to promote sparsity. Also, given that the slope coefficients, \markdownRendererDollarSign{}\markdownRendererBackslash{}beta\markdownRendererDollarSign{}, are random we can specify outside priors for the regressor slopes in case we're not able to get meaningful estimates from the historical data.\markdownRendererInterblockSeparator
{}The airline passenger dataset does not have any regressors, and so we'll fit a simple Bayesian structural model:\markdownRendererInterblockSeparator
{}\markdownRendererUlBegin
\markdownRendererUlItem 500 MCMC draws.\markdownRendererUlItemEnd 
\markdownRendererUlItem Use 1960 as the holdout period.\markdownRendererUlItemEnd 
\markdownRendererUlItem Trend and seasonality.\markdownRendererUlItemEnd 
\markdownRendererUlItem Forecast created by averaging across the MCMC draws. \markdownRendererUlItemEnd 
\markdownRendererUlItem Credible interval generated from the distribution of the MCMC draws.\markdownRendererUlItemEnd 
\markdownRendererUlItem Discarding the first MCMC iterations (burn-in).\markdownRendererUlItemEnd 
\markdownRendererUlEnd \markdownRendererInterblockSeparator
{}* Using a log transformation to make the model multiplicative\relax